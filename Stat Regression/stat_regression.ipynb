{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Regression Sept 2022 Rating\n",
    "Using the DES Job Seeker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import json\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from datetime import date, timedelta\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in participant characteristics from the stream_participants and jobseeker table from the DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap = pd.read_csv(\"C:/Users/AManalo/OneDrive - Arriba Group/Desktop/Star Ratings - AimBig/Stat Regression/ap_stat_variables.csv\") # stream_participants + jobseeker \n",
    "dis_type = pd.read_csv(\"C:/Users/AManalo/OneDrive - Arriba Group/Desktop/Star Ratings - AimBig/Stat Regression/participant_disability_type.csv\") # mental/physical disability mapping\n",
    "dis_type.columns = map(str.upper, dis_type.columns)\n",
    "\n",
    "# capitalising the column names in df_ap: \n",
    "df_ap = df_ap.rename(columns={'ess_identifier' : 'JOB_SEEKER_ID'})\n",
    "df_ap.columns = map(str.upper, df_ap.columns)\n",
    "\n",
    "df_ap['JOB_SEEKER_ID'] = pd.to_numeric(df_ap['JOB_SEEKER_ID'], errors='coerce')\n",
    "df_ap = df_ap[df_ap['JOB_SEEKER_ID'].isna() == False]\n",
    "\n",
    "df_ap['DISABILITY_TYPE_CODE'] = df_ap['DISABILITY_TYPE_CODE'].astype('object')\n",
    "dis_type['DISABILITY_TYPE_CODE'] = dis_type['DISABILITY_TYPE_CODE'].astype('object')\n",
    "# joining the disability type to the df_ap:\n",
    "df_ap = df_ap.merge(dis_type, on=['DISABILITY_TYPE_CODE'])\n",
    "\n",
    "\n",
    "\n",
    "df = df_ap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naVal = df.isna().sum() * 100 / len(df)\n",
    "missValDf = pd.DataFrame({'column_name' : df.columns, 'percent_missing' : naVal})\n",
    "missValDf.sort_values(by=['percent_missing'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NA values for JOB_SEEKER_ID and AGE_AT_EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Job_Seeker_Ids for the Dec 2020 star ratings - importing from the AimBig Star Prediction notebook - IDs that are eligible for 13 week outcomes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_13_pre.csv\")\n",
    "df_13 = df_13.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_13['JOB_SEEKER_ID'] = df_13['JOB_SEEKER_ID'].astype('int64')\n",
    "\n",
    "print(str(len(df_13)) + ': 13 outcomes len')\n",
    "\n",
    "df_13['JOB_SEEKER_ID'] = df_13['JOB_SEEKER_ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13['JOB_SEEKER_ID'].nunique()\n",
    "df['JOB_SEEKER_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.rename(columns={'TYPE' : 'DISABILITY_TYPE'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large reduction in data size is due to blank values in Num_13 and Den_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[['DATE_OF_BIRTH', 'NATIONALITY', 'COUNTRY_OF_BIRTH', 'EDUCATION',\n",
    "       'EDUCATION_CODE', 'CURRENT_CAPACITY_HOURS', 'JOB_SEEKER_ID', 'GENDER',\n",
    "       'INTERPRETER_LANGUAGE', 'INTERPRETER_LANGUAGE_CODE',\n",
    "       'FUNDING_LEVEL_CODE', 'RESIDENTIAL_ADDRESS_LINES',\n",
    "       'RESIDENTIAL_ADDRESS_SUBURB', 'RESIDENTIAL_ADDRESS_STATE', 'RESIDENTIAL_ADDRESS_POSTCODE',\n",
    "       'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'RATE_TYPE', 'STATUS', 'CENTRELINK_OUTCOME_CODE',\n",
    "       'ALLOWANCE_STARTED_ON', 'ALLOWANCE_TYPE', 'ALLOWANCE_RATE',\n",
    "       'TIME_IN_PLACEMENT', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM',\n",
    "       'TIME_IN_SITE', 'TIME_IN_PROVIDER', 'MODERATE_INTELLECTUAL_DISABILITY',\n",
    "       'DISABILITY_TYPE', 'EMPLOYMENT_BENCHMARK']] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing nationality, education, education_code, funding_level_code, centrelink_outcome_code, allowance_started_on, time_in_placement, disability_type_code, employment_benchmark \n",
    "- Ref to 'ap_stream_encoded_missing_reasons' excel file for reasons of exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.drop(columns=['NATIONALITY', 'EDUCATION', 'EDUCATION_CODE', 'FUNDING_LEVEL_CODE', 'CENTRELINK_OUTCOME_CODE', 'ALLOWANCE_STARTED_ON', 'TIME_IN_PLACEMENT', 'EMPLOYMENT_BENCHMARK', 'ALLOWANCE_TYPE', 'ALLOWANCE_RATE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categorical variables \n",
    "- Binary encoding for Y,N variables\n",
    "- JSCI_PERSONAL_FACTORS_OUTCOME: Nil, Low, Medium, High (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disability_type: 0 for Mental, 1 for Physical\n",
    "df_sub['DISABILITY_TYPE'] = df_sub['DISABILITY_TYPE'].dropna()\n",
    "df_sub['DISABILITY_TYPE'] = np.where(df_sub['DISABILITY_TYPE'] == \"mental\", 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['COUNTRY_OF_BIRTH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['DATE_OF_BIRTH', 'COUNTRY_OF_BIRTH',\n",
    "       'CURRENT_CAPACITY_HOURS', 'JOB_SEEKER_ID', 'GENDER',\n",
    "       'INTERPRETER_LANGUAGE', 'INTERPRETER_LANGUAGE_CODE',\n",
    "       'RESIDENTIAL_ADDRESS_LINES', 'RESIDENTIAL_ADDRESS_SUBURB',\n",
    "       'RESIDENTIAL_ADDRESS_STATE', 'RESIDENTIAL_ADDRESS_POSTCODE',\n",
    "       'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE',\n",
    "       'TIME_IN_PROVIDER', 'MODERATE_INTELLECTUAL_DISABILITY']\n",
    "\n",
    "\n",
    "for x in col_names:\n",
    "    print('\\n ' + str(x) + '\\n', df_sub[x].value_counts())\n",
    "\n",
    "# subsetting the features:\n",
    "features = ['DATE_OF_BIRTH', 'COUNTRY_OF_BIRTH',\n",
    "       'CURRENT_CAPACITY_HOURS', 'JOB_SEEKER_ID', 'GENDER',\n",
    "       'INTERPRETER_LANGUAGE', 'INTERPRETER_LANGUAGE_CODE',\n",
    "       'RESIDENTIAL_ADDRESS_LINES', 'RESIDENTIAL_ADDRESS_SUBURB',\n",
    "       'RESIDENTIAL_ADDRESS_STATE', 'RESIDENTIAL_ADDRESS_POSTCODE',\n",
    "       'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE',\n",
    "       'TIME_IN_PROVIDER', 'MODERATE_INTELLECTUAL_DISABILITY']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding the Age from DATE_OF_BIRTH - Taking the AGE column \n",
    "- Age is from the end period of the Dec 2020 star ratings - at 8/01/21\n",
    "- Dropping rows with more than 10 NULL column values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.dropna(thresh=10, axis=0) \n",
    "\n",
    "df_sub['DATE_OF_BIRTH'] = pd.to_datetime(df_sub['DATE_OF_BIRTH']).dt.date\n",
    "\n",
    "end_date = datetime.strptime('2022/10/07', '%Y/%m/%d')\n",
    "end_date_year = end_date.year\n",
    "end_date_month = end_date.month\n",
    "end_date_day = end_date.day\n",
    "\n",
    "df_sub['AGE'] = df_sub['DATE_OF_BIRTH'].apply(lambda x: end_date_year - x.year - ((end_date_month, end_date_day) < (x.month, x.day)))\n",
    "# converting Age -> INT\n",
    "df_sub['AGE'] = df_sub['AGE'].astype('int64')\n",
    "df_sub[['DATE_OF_BIRTH', 'AGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sub['INTERPRETER_LANGUAGE'] = df_sub['INTERPRETER_LANGUAGE'].fillna('NOT APPLICABLE')\n",
    "df_sub['INTERPRETER_LANGUAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['MODERATE_INTELLECTUAL_DISABILITY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[['TIME_IN_STREAM',\n",
    "       'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER']]\n",
    "\n",
    "df_sub['TIME_IN_STREAM'].isna().sum() # drop missing values (only a few)\n",
    "df_sub = df_sub[df_sub['TIME_IN_STREAM'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['TIME_IN_PROVIDER'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['TIME_IN_SITE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'].isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_SITE'\n",
    "- WAGE_SUBSIDY_ELIGIBLE: encoded by maintaining the proportion of 1 to 0s \n",
    "- TIME_IN_SITE: encoded by the normal distribution of the values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plotting the distribution of WAGE_SUBSIDY_ELIGIBLE, TIME_IN_SITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.boxplot(column=['TIME_IN_SITE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of TIME_IN_SITE is slightly left skewed, replace missing values with MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time_site = df_sub['TIME_IN_SITE'].mean()\n",
    "df_sub['TIME_IN_SITE'] = df_sub['TIME_IN_SITE'].fillna(mean_time_site)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing missing values in WAGE_SUBSIDY_ELIGIBLE with the proportion of existing values\n",
    "### Part done MANUALLY in EXCEL -> filter on NULL values on WAGE_SUBSIDY_ELIGIBLE + rate_type (wage) + status (approved, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validRates = ['DES-DMS Wage Start Subsidy Payment', 'DES-DMS Wage Subsidy Fee', 'DES-ESS Wage Start Subsidy Payment', 'DES-ESS Wage Subsidy Fee', 'Restart Wage Subsidy DESA Payment', 'Restart Wage Subsidy DESB Payment', 'Restart Wage Subsidy DESB Payment', 'Restart Wage Subsidy Special Claim-DMS Job Seeker', 'Restart Wage Subsidy Special Claim-ESS Job Seeker']\n",
    "validStatus = ['Approved', 'Pending', 'Lodged']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'].isna().sum() #-> replace with 0 -> assuming the manual replacement in EXCEL has been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['WAGE_SUBSIDY_ELIGIBLE'] = df_sub['WAGE_SUBSIDY_ELIGIBLE'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the columns based on distinct values -->\n",
    "\n",
    "df_sub['COUNTRY_OF_BIRTH'].isna().sum()\n",
    "# 24 na's -> drop rows\n",
    "df_sub = df_sub[df_sub['COUNTRY_OF_BIRTH'].notna()]\n",
    "\n",
    "\n",
    "## INTERPRETER_LANGUAGE: if it is 'NOT APPLICABLE','NaN', 'Sign Language' -> then encode '0', otherwise '1'\n",
    "no_inter = ['NOT APPLICABLE', 'NaN', 'Sign Language', '*', 'NULL']\n",
    "df_sub['INTERPRETER_LANGUAGE'] = np.where(df_sub['INTERPRETER_LANGUAGE'].isin(no_inter), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "# CULTURAL_linguist from 'COUNTRY_OF_BIRTH', 'INTERPRETER_LANGUAGE'\n",
    "# if country_of_birth is NOT Australia, then 1 otherwise 0:\n",
    "df_sub['COUNTRY_DIVERSE'] = np.where(df_sub['COUNTRY_OF_BIRTH'] != 'Australia', 1, 0)\n",
    "df_sub['CULTURAL_LINGUIST'] = np.where((df_sub['COUNTRY_DIVERSE'] == 1) & (df_sub['INTERPRETER_LANGUAGE'] == 1), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## CURRENT_CAPACITY_HOURS: encode Aug-14 -> \"8-14\" range - but taking the midpoint  (and check with Depar why it would be Aug-14, should we use imputation?), remove NaN, remove UNKNOWN(count = 6)\n",
    "# integer encoding with Midpoint (rounded down)\n",
    "\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('Aug-14', '8-14')\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('NaN', 0)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('0-7', 3)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('8+', 8)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('8-14', 11)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('15-22', 18)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('23-29', 26)\n",
    "df_sub['CURRENT_CAPACITY_HOURS'] = df_sub['CURRENT_CAPACITY_HOURS'].replace('30+', 30)\n",
    "df_sub = df_sub[df_sub['CURRENT_CAPACITY_HOURS'] != 'UNNOWN']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Gender: 0 for Male, 1 for Female\n",
    "df_sub['GENDER'] = df_sub['GENDER'].replace('Female', 1)\n",
    "df_sub['GENDER'] = df_sub['GENDER'].replace('Male', 0)\n",
    "\n",
    "\n",
    "# Type conversions --\n",
    "## INDIGENOUS_IND: Check that type is numeric \n",
    "## WAGE_SUBSIDY_ELIGIBLE: check type is numeric \n",
    "\n",
    "dict_types = {'INDIGENOUS_IND' : 'int', 'WAGE_SUBSIDY_ELIGIBLE' : 'int'}\n",
    "\n",
    "df_sub = df_sub.astype(dict_types)\n",
    "\n",
    "## MODERATE_INTELLECTUAL_DISABILITY - should include with 'DISABILITY_TYPE_CODE'? Or separately? - encode True, False with 0 and 1\n",
    "df_sub['MODERATE_INTELLECTUAL_DISABILITY'] = np.where(df_sub['MODERATE_INTELLECTUAL_DISABILITY'] == False, 0, 1) \n",
    "# need to drop the 1\n",
    "df_sub = df_sub[df_sub['MODERATE_INTELLECTUAL_DISABILITY'] != 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['INTERPRETER_LANGUAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['CURRENT_CAPACITY_HOURS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[['RESIDENTIAL_ADDRESS_LINES', 'RESIDENTIAL_ADDRESS_SUBURB', 'RESIDENTIAL_ADDRESS_POSTCODE',\n",
    "       'RESIDENTIAL_ADDRESS_STATE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the metro postcodes for each state\n",
    "- Just using postcode and state\n",
    "- Cleaning the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['RESIDENTIAL_ADDRESS_STATE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '*' in state\n",
    "df_sub = df_sub[df_sub['RESIDENTIAL_ADDRESS_STATE'] != '*']\n",
    "df_sub['RESIDENTIAL_ADDRESS_STATE'].value_counts()\n",
    "\n",
    "# convert postcode to int\n",
    "df_sub['RESIDENTIAL_ADDRESS_POSTCODE'] = df_sub['RESIDENTIAL_ADDRESS_POSTCODE'].astype('int64')\n",
    "\n",
    "df_sub = df_sub[df_sub['GENDER'] != 'Unknown'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metro postcodes for each state\n",
    "source: https://support.shippit.com/hc/en-us/articles/4403703156377-Rule-to-Allocate-a-Courier-for-Metro-and-Regional-Destinations\n",
    "\n",
    "Source: http://www.impactlists.com.au/ImpactLists/media/list-tools/Useful-Postcode-Ranges.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDIGENOUS_STATUS and METRO LOCATION -> check address postcode AND state\n",
    "df_sub['METRO'] = False # initialise the metro column to False values\n",
    "\n",
    "\n",
    "# ind = 0 (start bounds), ind=1 (end bounds)\n",
    "nsw_met = [[1000, 1920], [2000, 2239], [2555, 2574], [2740, 2786]] # sydney\n",
    "qld_met = [[2484, 2494], [4000, 4370], [4373, 4381], [4400, 4405], [4500, 4580], [4600, 4610], [4614, 4618], [9000, 9919]] #brisbane\n",
    "vic_met = [[3000, 3210], [3335, 3341], [3425, 3443], [3750, 3811], [3910, 3920], [3926, 3944], [3972, 3978], [3980, 3983], [8000, 8899]] #Melbourne\n",
    "wa_met = [[6000, 6214], [6800, 6999]] #perth\n",
    "tas_met = [[7000,7010], [7249,7250]] # hobart\n",
    "sa_met = [[5000, 5199], [5800, 5999]] #adelaide\n",
    "nt_met = [[800, 820], [900, 910]] # darwin\n",
    "act_met = [[200,299], [2600, 2620], [2900,2920]] #canberra\n",
    "\n",
    "\n",
    "\n",
    "# creating the \"METRO\" column - 1 if metro postcode, 0 otherwise\n",
    "def metro_postcode(postcode_list, postcode):\n",
    "    is_metro = False # initialise boolean \n",
    "    i = 0\n",
    "    while i < len(postcode_list):\n",
    "        start_bounds = int(postcode_list[i][0])\n",
    "        end_bounds = int(postcode_list[i][1])\n",
    "        if (postcode >= start_bounds) & ( postcode <= end_bounds):\n",
    "            is_metro = True\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return is_metro\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# passing the postcodes to the 'metro_postcode' function ->\n",
    "for i, row in df_sub.iterrows():\n",
    "    x = df_sub['RESIDENTIAL_ADDRESS_POSTCODE'][i] #postcode\n",
    "    if x is None:\n",
    "        pass \n",
    "    else: \n",
    "        # getting the state\n",
    "        state = df_sub['RESIDENTIAL_ADDRESS_STATE'][i]\n",
    "        if state == 'NSW':\n",
    "            df_sub['METRO'][i] = metro_postcode(nsw_met, x)\n",
    "        elif state == 'QLD':\n",
    "            df_sub['METRO'][i] = metro_postcode(qld_met, x)\n",
    "        elif state == 'VIC':\n",
    "            df_sub['METRO'][i] = metro_postcode(vic_met, x)\n",
    "        elif state == 'WA':\n",
    "            df_sub['METRO'][i] = metro_postcode(wa_met, x)\n",
    "        elif state == 'TAS':\n",
    "            df_sub['METRO'][i] = metro_postcode(tas_met, x)\n",
    "        elif state == 'SA':\n",
    "            df_sub['METRO'][i] = metro_postcode(sa_met, x)\n",
    "        elif state == 'NT':\n",
    "            df_sub['METRO'][i] = metro_postcode(nt_met, x)\n",
    "        elif state == 'ACT':\n",
    "            df_sub['METRO'][i] = metro_postcode(act_met, x)\n",
    "        else: \n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[[\"RESIDENTIAL_ADDRESS_POSTCODE\", \"RESIDENTIAL_ADDRESS_STATE\", \"METRO\"]].head(5)\n",
    "# encode True -> 1, False -> 0\n",
    "df_sub[\"METRO\"] = np.where(df_sub[\"METRO\"] == True, 1, 0)\n",
    "\n",
    "# filtering for INDIGENOUS_INDICATOR = 1 and METRO = 1\n",
    "df_sub['IND_METRO'] = np.where((df_sub['INDIGENOUS_IND'] == 1) & (df_sub['METRO'] == 1), 1, 0) \n",
    "\n",
    "df_sub[[\"RESIDENTIAL_ADDRESS_POSTCODE\", \"RESIDENTIAL_ADDRESS_STATE\", \"METRO\", \"INDIGENOUS_IND\", \"IND_METRO\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values again: \n",
    "naVal = df_sub.isna().sum() * 100 / len(df)\n",
    "missValDf = pd.DataFrame({'column_name' : df_sub.columns, 'percent_missing' : naVal})\n",
    "missValDf.sort_values(by=['percent_missing'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the current_capacity_hours missing values\n",
    "df_sub = df_sub[df_sub['CURRENT_CAPACITY_HOURS'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naVal = df_sub.isna().sum() * 100 / len(df)\n",
    "missValDf = pd.DataFrame({'column_name' : df_sub.columns, 'percent_missing' : naVal})\n",
    "missValDf.sort_values(by=['percent_missing'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final variables subset for logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CURRENT_CAPACITY_HOURS', 'GENDER', 'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER',\n",
    "       'MODERATE_INTELLECTUAL_DISABILITY', 'AGE', 'COUNTRY_DIVERSE', 'CULTURAL_LINGUIST', 'IND_METRO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying strong predictors for numerating participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using StandardScalar to standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "features = ['CURRENT_CAPACITY_HOURS', 'GENDER', 'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER',\n",
    "       'MODERATE_INTELLECTUAL_DISABILITY', 'AGE', 'COUNTRY_DIVERSE', 'CULTURAL_LINGUIST', 'IND_METRO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "Between numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether there exists variable with all 0s or 1s -> to avoid PerfectSeparation error in the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in df_sub:\n",
    "#    print(col , df_sub[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_sub[['CURRENT_CAPACITY_HOURS', 'GENDER', 'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER',\n",
    "       'MODERATE_INTELLECTUAL_DISABILITY', 'AGE', 'COUNTRY_DIVERSE', 'CULTURAL_LINGUIST', 'IND_METRO']]\n",
    "df_feat.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the NaN correlation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat =  df_sub[['CURRENT_CAPACITY_HOURS', 'GENDER', 'DISABILITY_TYPE', 'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER',\n",
    "       'AGE', 'COUNTRY_DIVERSE', 'CULTURAL_LINGUIST', 'IND_METRO']]\n",
    "\n",
    "corr_matrix = df_feat.corr()\n",
    "corr_matrix.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships between Features Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_feat_num =  df_sub[['CURRENT_CAPACITY_HOURS', 'GENDER', 'DISABILITY_TYPE', 'INDIGENOUS_IND', 'WAGE_SUBSIDY_ELIGIBLE', 'TIME_IN_STREAM', 'TIME_IN_PROGRAM', 'TIME_IN_SITE', 'TIME_IN_PROVIDER',\n",
    "       'AGE', 'COUNTRY_DIVERSE', 'CULTURAL_LINGUIST', 'IND_METRO']]\n",
    "sns.pairplot(data = df_feat_num, diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasons for Low Accuracy in Model (13 week outcomes)\n",
    "from analysis of sns pairplot:\n",
    "\n",
    "- Across the diagonal where each feature is plotted against itself, the distribution of each feature differs.  \n",
    "- IND_METRO: no strong relations to Num_13. When indicator is 0 and 1, Num_13 = 0, and left skewed \n",
    "- CULTURAL_LINGUIST: no strong relation, and left skewed\n",
    "- COUNTRY_DIVERSE: no strong relation \n",
    "- None of the variables are strong indicators for Num_13. \n",
    "However, we can reduce correlation between the variables and reduce dimensionality. \n",
    "Note: Since Num_13 is imbalanced with Num_13 = 1 as the majority class -> then there may be a slight right-hand skew against the features \n",
    "\n",
    "- TIME_IN_PROGRAM and TIME_IN_STREAM -> positive correlation (REMOVE ONE)\n",
    "- TIME_IN_SITE and TIME_IN_PROVIDER -> positive correlation (REMOVE ONE)\n",
    "- AGE is right skewed\n",
    "- Two peaks (0, 1) with binary features \n",
    "- WAGE_SUBSIDY_ELIGIBLE -> left skewed\n",
    "- INDIGEOUS_IND -> left skewed\n",
    "\n",
    "Subsetting the features and reducing dependency between these. Final subset: \n",
    "\n",
    "features = ['CURRENT_CAPACITY_HOURS', 'GENDER', 'INDIGENOUS_IND', 'DISABILITY_TYPE', 'TIME_IN_PROGRAM', 'TIME_IN_SITE',\n",
    "       'AGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression Model (Num_13 = 0 or 1)\n",
    "Dependent var: 0 or 1 (Num_13)\n",
    "Independent: features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'AGE', 'IND_METRO']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across Diagonal -> correct predictions for Binary output (0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f1-score (comb. of precision and recall is 0.81 for both classes)\n",
    "- support = no. of actual occurrences of the class in the specified dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LG equation - for a person to record an outcome (Num_13 = 1):\n",
    "\n",
    "logit(p) = -0.154(AGE_AT_EXTRACT) - 1.394(WEEKS_REGISTERED) - 1.184(TEMP_CAPACITY_2_HOURS) + 0.375(CURRENT_CAPACITY_HOURS) - 0.446(FUTURE_CAPACITY_INTERVENTION_HOURS) -0.359(JSCI_PERSONAL_FACTORS_OUTCOME) - 1.536(CALD) - 0.348\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Regression Function (13, 26, 52 week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve performance - GridSearch for hyperparameter tuning for Logistic Regression model \n",
    "\n",
    "### Passing each job seeker row -> eval regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13['Num_13'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 233/1030\n",
    "x # defines the ratio of class weights\n",
    "1 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def eval_regression(df_n, df_apj, features, n_type):\n",
    "    '''Evaluation of the statistical regression fuction. Passes each performance measure dataframe (from pre-quarterisation) and predicts whether a participant numerates based on\n",
    "    demographic variables. Returns the classification metric report, confusion matrix and the logreg equation. Input: takes df_n (df_13, df_26, or df_52), and df_apj (stream_participant and jobseeker), 'n' : string of the performance measure,\n",
    "    df_upsampled: dataframe for the upsampled minority class of df_n (if class imbalance - otherwise None).\n",
    "    df_apj is the equivalent to the df_sub that has been pre-processed prior and outside of the function'''\n",
    "    df_n = df_n.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "    df_n['JOB_SEEKER_ID'] = df_n['JOB_SEEKER_ID'].astype('int64')\n",
    "    # merging df_n and df_apj:\n",
    "    \n",
    "    df_merge = df_apj.merge(df_n, on=['JOB_SEEKER_ID'], how='inner')\n",
    "    df_merge['JOB_SEEKER_ID'] = df_merge['JOB_SEEKER_ID'].astype('int64')\n",
    "    # check class imbalance:\n",
    "    df_sub = df_merge\n",
    "    num_n = str(n_type)\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    # running the Binary Logistic Regression (Num_n = 0, or Num_n = 1)\n",
    "    #separation of the features:\n",
    "    x = df_sub.loc[:, features].values\n",
    "    # target (Num_13)\n",
    "    y = df_sub[num_n]\n",
    "    \n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16, stratify=y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=1, random_state=12)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # scaling the features\n",
    "    X_train = scaler.fit_transform(X_resampled)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "    # HYPERPARAMETER TUNING with GridSearchCV\n",
    "    params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    clf = LogisticRegression(penalty='l2', class_weight='balanced', random_state=42)\n",
    "\n",
    "\n",
    "    folds = 5\n",
    "    model_cv = GridSearchCV(estimator = clf, \n",
    "                            param_grid = params, \n",
    "                            scoring= 'accuracy', \n",
    "                            cv = folds,\n",
    "                            return_train_score=True,\n",
    "                            verbose = 3)\n",
    "\n",
    "    model_cv.fit(X_resampled, y_resampled) # fit on training\n",
    "    # getting the best hyper-parameter\n",
    "    C = model_cv.best_params_\n",
    "    hyperVal = C.get('C')\n",
    "\n",
    "\n",
    "\n",
    "    # define model \n",
    "    logreg = LogisticRegression(C=hyperVal, penalty='l2', class_weight='balanced', random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    # fit the model and predict\n",
    "    model = logreg.fit(X_resampled, y_resampled)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Adjust the decision threshold\n",
    "    threshold = 0.5  # You can adjust this value based on your needs\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # confusion matrix:\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #printing test set results\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # print the AUC:\n",
    "    y_pred_proba = logreg.predict_proba(X_train)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_resampled,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_resampled, y_pred_proba)\n",
    "    ### AUC - COMMENTED OUT\n",
    "    #plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    #plt.legend(loc=4)\n",
    "    #plt.show()\n",
    "    \n",
    "\n",
    "    # the logistic regression equation \n",
    "    \n",
    "    coef = model.coef_\n",
    "    coef = list(coef)\n",
    "    intercept = model.intercept_\n",
    "    df_merge = df_merge[['JOB_SEEKER_ID','CURRENT_CAPACITY_HOURS', 'GENDER', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE', 'INDIGENOUS_IND', 'IND_METRO', 'WAGE_SUBSIDY_ELIGIBLE']]\n",
    "    type_dict = {'CURRENT_CAPACITY_HOURS' : float, 'GENDER' : float, 'TIME_IN_SITE' : float, 'DISABILITY_TYPE' : float, 'AGE' : float, 'INDIGENOUS_IND' : float, 'IND_METRO' : float}\n",
    "    df_merge = df_merge.astype(type_dict)\n",
    "    return cnf_matrix, class_report, coef, intercept, df_merge, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13, 26, 52 week stat regression outcomes\n",
    "\n",
    "1. Model coefficients would differ based on each performance measure\n",
    "2. Generate the model coefficients, get from the eval_regression function. Create a formula.\n",
    "3. Pass each performance measure dataframe (df_n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 week outcomes to regression function\n",
    "- Using model_13 -> pass each row of the dataframe to the model and call \"model.pred(row)\" -> set this to expected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['CURRENT_CAPACITY_HOURS', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_13, class_report_13, coef_13, intercept_13, df_merge_13, model_13 = eval_regression(df_13, df_sub, features, 'Num_13')\n",
    "print(class_report_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_report_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "def knn_regression(df_n, df_apj, features, n_type):\n",
    "    '''Evaluation of the statistical regression fuction. Passes each performance measure dataframe (from pre-quarterisation) and predicts whether a participant numerates based on\n",
    "    demographic variables. Returns the classification metric report, confusion matrix and the logreg equation. Input: takes df_n (df_13, df_26, or df_52), and df_apj (stream_participant and jobseeker), 'n' : string of the performance measure,\n",
    "    df_upsampled: dataframe for the upsampled minority class of df_n (if class imbalance - otherwise None).\n",
    "    df_apj is the equivalent to the df_sub that has been pre-processed prior and outside of the function'''\n",
    "    df_n = df_n.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "    df_n['JOB_SEEKER_ID'] = df_n['JOB_SEEKER_ID'].astype('int64')\n",
    "    # merging df_n and df_apj:\n",
    "    \n",
    "    df_merge = df_apj.merge(df_n, on=['JOB_SEEKER_ID'], how='inner')\n",
    "    df_merge['JOB_SEEKER_ID'] = df_merge['JOB_SEEKER_ID'].astype('int64')\n",
    "    # check class imbalance:\n",
    "    df_sub = df_merge\n",
    "    num_n = str(n_type)\n",
    "   \n",
    "    \n",
    "    # running the Binary Logistic Regression (Num_n = 0, or Num_n = 1)\n",
    "    #separation of the features:\n",
    "    x = df_sub.loc[:, features].values\n",
    "    # target (Num_13)\n",
    "    y = df_sub[num_n]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\n",
    "    rus = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train) # only resample after train/test split\n",
    "    # creating a balanced training dataset -> # scale features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_resampled)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_resampled = X_train  \n",
    "    \n",
    "\n",
    "    \n",
    "    # define model \n",
    "    knn_model = KNeighborsClassifier()\n",
    "\n",
    "    # create the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(1, 21),\n",
    "        'metric': ['minkowski'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "    # create the cross-validation method\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "    # perform the grid search\n",
    "    grid_search = GridSearchCV(knn_model, param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "     # predict on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # print the classification report\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return best_model, X_train, X_test, y_resampled, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['WAGE_SUBSIDY_ELIGIBLE', 'CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_13, X_train, X_test, y_resampled, y_test, scaler = knn_regression(df_13, df_sub, features, 'Num_13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the appropriate values for k \n",
    "Through exploration of the elbow graph, the ideal number of neighbours with the lowest RMSE is ~8. Neighbours at 15 generates a lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating expected outcomes for 13 week \n",
    "- Addition of a 'Num_13_exp' -> Numerator for 13 week EXPECTED column in the \"df_13\" one\n",
    "- Using model_13 -> pass each row of the dataframe to the model and call \"model.pred(row)\" -> set this to expected \n",
    "- Passing all rows (1263) of df_13 t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set df_13['JOB_SEEKER_ID'] as the index\n",
    "df_13 = df_13.set_index('JOB_SEEKER_ID')\n",
    "# create Num_13_exp column\n",
    "df_13['Num_13_exp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = df_13.drop(columns={'Unnamed: 0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_13 with df_merge_13\n",
    "df_13_final = df_merge_13.merge(df_13, on=['JOB_SEEKER_ID'], how='inner')\n",
    "df_13_final = df_13_final.rename(columns={'Num_13' : 'Num_13_full', 'Den_13' : 'Den_13_full'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_final = df_13_final[[\"JOB_SEEKER_ID\", \"Num_13_full\", \"Den_13_full\"]]\n",
    "df_13 = df_13_final\n",
    "df_13['Num_13_exp'] = 0 #initilalise expected outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the class imbalance -> resample, when predicting, to offset the prediction of all 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output file for 13 week expected outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13.to_csv('df_13_exp.csv')\n",
    "\n",
    "df_13['Num_13_exp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging df_n with df_merge_n - to get common rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the 26 week outcome to the stat_regression function \n",
    "- Creating 3 subset dataframes for 3 performance subtypes: Full, Pathway, Work Assist/Bonus\\\n",
    "- Removing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26 week outcome\n",
    "df_26 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_26_new.csv\")\n",
    "df_26_full = df_26[[\"Job_Seeker_ID\", \"Num_26_full\", \"Den_26_full\"]]\n",
    "df_26_full = df_26_full.dropna()\n",
    "df_26_path = df_26[[\"Job_Seeker_ID\", \"Num_26_path\", \"Den_26_path\"]]\n",
    "df_26_path = df_26_path.dropna()\n",
    "df_26_wrkast = df_26[[\"Job_Seeker_ID\", \"Num_26_wrkast\", \"Den_26_wrkast\"]]\n",
    "df_26_wrkast = df_26_wrkast.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_full['Num_26_full'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_path['Num_26_path'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_wrkast['Num_26_wrkast'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_26, class_report_26, coef_26, intercept_26, df_merge_26, model_26 = eval_regression(df_26_full, df_sub, features, 'Num_26_full')\n",
    "\n",
    "\n",
    "print(\"26 week FULL outcome\" + '\\n')\n",
    "print(cnf_matrix_26)\n",
    "print(class_report_26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since there is only one class for 26 week work assist/pathway - use the model for the full outcome for prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 26 week full outcome (expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_full = df_26_full.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "# merge with df_merge_26\n",
    "df_26_full_m = df_merge_26.merge(df_26_full, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_merge_26 = df_26_full_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\"]]\n",
    "\n",
    "df_merge_26 = df_merge_26.set_index('JOB_SEEKER_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_full_m\n",
    "df_26_full = df_26_full_m[[\"JOB_SEEKER_ID\", \"Num_26_full\", \"Den_26_full\"]]\n",
    "df_26_full['Num_26_full_exp'] = 0\n",
    "df_26_full = df_26_full.set_index('JOB_SEEKER_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_merge_26.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'GENDER', 'TIME_IN_SITE', 'DISABILITY_TYPE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "    if index == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_26_full.loc[index].Num_26_full_exp = model_26.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing 52 performance measure to regression function\n",
    "- Since only 1 class in 52 week outcome -> using the 26 week prediction function \n",
    "- Replace line 9 when more data comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 52 week full outcome (expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_52 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_52_new.csv\")\n",
    "df_52_full = df_52[[\"Job_Seeker_ID\", \"Num_52_full\", \"Den_52_full\"]]\n",
    "\n",
    "df_52_path = df_52[[\"Job_Seeker_ID\", \"Num_52_path\", \"Den_52_path\"]]\n",
    "\n",
    "\n",
    "df_52_full = df_52_full.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "# merge with df_merge_52\n",
    "df_52_full_m = df_sub.merge(df_52_full, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_merge_52 = df_52_full_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\"]]\n",
    "\n",
    "df_merge_52 = df_merge_52.set_index('JOB_SEEKER_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for regression - To-Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def rf_model(df_n, df_apj, features, n_type):\n",
    "    '''Evaluation of the statistical regression fuction. Passes each performance measure dataframe (from pre-quarterisation) and predicts whether a participant numerates based on\n",
    "    demographic variables. Returns the classification metric report, confusion matrix and the logreg equation. Input: takes df_n (df_13, df_26, or df_52), and df_apj (stream_participant and jobseeker), 'n' : string of the performance measure,\n",
    "    df_upsampled: dataframe for the upsampled minority class of df_n (if class imbalance - otherwise None).\n",
    "    df_apj is the equivalent to the df_sub that has been pre-processed prior and outside of the function'''\n",
    "    df_n = df_n.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "    df_n['JOB_SEEKER_ID'] = df_n['JOB_SEEKER_ID'].astype('int64')\n",
    "    # merging df_n and df_apj:\n",
    "    \n",
    "    df_merge = df_apj.merge(df_n, on=['JOB_SEEKER_ID'], how='inner')\n",
    "    df_merge['JOB_SEEKER_ID'] = df_merge['JOB_SEEKER_ID'].astype('int64')\n",
    "    # check class imbalance:\n",
    "    df_sub = df_merge\n",
    "    num_n = str(n_type)\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    # running the Binary Logistic Regression (Num_n = 0, or Num_n = 1)\n",
    "    #separation of the features:\n",
    "    x = df_sub.loc[:, features].values\n",
    "    # target (Num_13)\n",
    "    y = df_sub[num_n]\n",
    "    \n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16, stratify=y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\n",
    "    \n",
    "    adasyn = ADASYN(sampling_strategy=1, random_state=12)\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "  \n",
    "    # Standardize the training and testing datasets\n",
    "    X_train = scaler.fit_transform(X_resampled)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        # HYPERPARAMETER TUNING with RandomizedSearchCV\n",
    "    params = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, 40, 50, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Using RandomizedSearchCV for a faster search\n",
    "    model_cv = RandomizedSearchCV(estimator=clf, \n",
    "                                  param_distributions=params, \n",
    "                                  n_iter=50,\n",
    "                                  scoring='accuracy', \n",
    "                                  cv=5,\n",
    "                                  verbose=3, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    model_cv.fit(X_resampled, y_resampled)\n",
    "    best_params = model_cv.best_params_\n",
    "\n",
    "    # define model\n",
    "    rf_model = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                      max_features=best_params['max_features'],\n",
    "                                      max_depth=best_params['max_depth'],\n",
    "                                      min_samples_split=best_params['min_samples_split'],\n",
    "                                      min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                      bootstrap=best_params['bootstrap'],\n",
    "                                      random_state=42)\n",
    "\n",
    "    # fit the model and predict\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Perform cross-validation to evaluate the model\n",
    "    cv_scores = cross_val_score(rf_model, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    print('Cross-validation scores:', cv_scores)\n",
    "    print('Mean cross-validation score:', np.mean(cv_scores))\n",
    "    \n",
    "        # confusion matrix:\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #printing test set results\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    \n",
    "\n",
    "    # the logistic regression equation \n",
    "    \n",
    "  \n",
    "\n",
    "    df_merge = df_merge[['JOB_SEEKER_ID','CURRENT_CAPACITY_HOURS', 'GENDER', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE', 'INDIGENOUS_IND', 'IND_METRO']]\n",
    "    type_dict = {'CURRENT_CAPACITY_HOURS' : float, 'GENDER' : float, 'TIME_IN_SITE' : float, 'DISABILITY_TYPE' : float, 'AGE' : float, 'INDIGENOUS_IND' : float, 'IND_METRO' : float}\n",
    "    df_merge = df_merge.astype(type_dict)\n",
    "    \n",
    "    return cnf_matrix, class_report, df_merge, rf_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - 13, 26, 52 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub = ['CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_13, class_report_13, df_merge_13, rf_model_13 = rf_model(df_13, df_sub, features_sub, 'Num_13_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnf_matrix_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing each dataframe row of df_merge_13 to the model.pred()\n",
    "for index, row in df_merge_13.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    df_13.loc[index, 'Num_13_exp'] = rf_model_13.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13.Num_13_exp.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline\n",
    "def en_model(df_n, df_apj, features, n_type):\n",
    "    '''Evaluation of the statistical regression fuction. Passes each performance measure dataframe (from pre-quarterisation) and predicts whether a participant numerates based on\n",
    "    demographic variables. Returns the classification metric report, confusion matrix and the logreg equation. Input: takes df_n (df_13, df_26, or df_52), and df_apj (stream_participant and jobseeker), 'n' : string of the performance measure,\n",
    "    df_upsampled: dataframe for the upsampled minority class of df_n (if class imbalance - otherwise None).\n",
    "    df_apj is the equivalent to the df_sub that has been pre-processed prior and outside of the function'''\n",
    "    df_n = df_n.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "    df_n['JOB_SEEKER_ID'] = df_n['JOB_SEEKER_ID'].astype('int64')\n",
    "    # merging df_n and df_apj: Participants feature data with their num/den counts\n",
    "    \n",
    "    df_merge = df_apj.merge(df_n, on=['JOB_SEEKER_ID'], how='inner')\n",
    "    df_merge['JOB_SEEKER_ID'] = df_merge['JOB_SEEKER_ID'].astype('int64')\n",
    "    # check class imbalance:\n",
    "    df_sub = df_merge\n",
    "    num_n = str(n_type)\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    # running the Binary Logistic Regression (Num_n = 0, or Num_n = 1)\n",
    "    #separation of the features:\n",
    "    x = df_sub.loc[:, features].values\n",
    "    # target (Num_13)\n",
    "    y = df_sub[num_n]\n",
    "    \n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16, stratify=y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\n",
    "    \n",
    "    adasyn = ADASYN(sampling_strategy=1, random_state=12)\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy=1, random_state=12)\n",
    "\n",
    "    # Create pipeline to apply both over-sampling and under-sampling\n",
    "    pipeline = Pipeline([('over', adasyn), ('under', rus)])\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "  \n",
    "    # Standardize the training and testing datasets\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_resampled)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "     # Hyperparameter tuning for logistic regression\n",
    "    params = {'C':[0.01, 0.1, 1, 10, 100]}\n",
    "    clf = LogisticRegression(penalty='l2', random_state=42)\n",
    "    folds = 5\n",
    "    model_cv = GridSearchCV(estimator=clf, \n",
    "                            param_grid=params, \n",
    "                            scoring='accuracy', \n",
    "                            cv=folds,\n",
    "                            return_train_score=True,\n",
    "                            verbose=3)\n",
    "    model_cv.fit(X_resampled, y_resampled)\n",
    "    hyperVal = model_cv.best_params_['C']\n",
    "\n",
    "   \n",
    "    # Hyperparameter tuning for random forest\n",
    "    params = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, 40, 50, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    model_rf_cv = RandomizedSearchCV(estimator=clf, \n",
    "                                  param_distributions=params, \n",
    "                                  n_iter=50,\n",
    "                                  scoring='accuracy', \n",
    "                                  cv=5,\n",
    "                                  verbose=3, \n",
    "                                  random_state=42, \n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    model_rf_cv.fit(X_resampled, y_resampled)\n",
    "    best_params = model_rf_cv.best_params_\n",
    "\n",
    "    # Instantiate the individual models\n",
    "    logreg = LogisticRegression(C=hyperVal, penalty='l2', random_state=42)\n",
    "    rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                max_features=best_params['max_features'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                min_samples_split=best_params['min_samples_split'],\n",
    "                                min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                bootstrap=best_params['bootstrap'],\n",
    "                                random_state=42)\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    # Create the ensemble model using VotingClassifier\n",
    "    ensemble = VotingClassifier(estimators=[('lr', logreg), ('rf', rf), ('svm', svm)], voting='soft')\n",
    "\n",
    "    # Fit the ensemble model\n",
    "    ensemble.fit(X_resampled, y_resampled)\n",
    "    \n",
    "     # Make predictions\n",
    "    y_pred = ensemble.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "    \n",
    "    # Perform cross-validation to evaluate the model\n",
    "    cv_scores = cross_val_score(ensemble, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    print('Cross-validation scores:', cv_scores)\n",
    "    print('Mean cross-validation score:', np.mean(cv_scores))\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    df_merge = df_merge[['JOB_SEEKER_ID','CURRENT_CAPACITY_HOURS', 'GENDER', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE', 'INDIGENOUS_IND', 'IND_METRO']]\n",
    "    type_dict = {'CURRENT_CAPACITY_HOURS' : float, 'GENDER' : float, 'TIME_IN_SITE' : float, 'DISABILITY_TYPE' : float, 'AGE' : float, 'INDIGENOUS_IND' : float, 'IND_METRO' : float}\n",
    "    df_merge = df_merge.astype(type_dict)\n",
    "    \n",
    "    return cnf_matrix, class_report, df_merge, ensemble\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub = ['CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.6609589  0.66438356 0.6130137  0.64948454 0.60824742]\n",
      "Mean cross-validation score: 0.6392176246292897\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix_13, class_report_13, df_merge_13, en_model_13 = en_model(df_13, df_sub, features_sub, 'Num_13_full')\n",
    "# passing each dataframe row of df_merge_13 to the model.pred()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 98  99]\n",
      " [157 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.50      0.43       197\n",
      "           1       0.61      0.50      0.55       313\n",
      "\n",
      "    accuracy                           0.50       510\n",
      "   macro avg       0.50      0.50      0.49       510\n",
      "weighted avg       0.52      0.50      0.50       510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cnf_matrix_13)\n",
    "print(class_report_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Model - Another Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1698\n",
       "Name: Num_13_exp, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_merge_13.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    df_13.loc[index, 'Num_13_exp'] = en_model_13.predict(X_test)\n",
    "\n",
    "df_13.Num_13_exp.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def xgboost_model(df_n, df_apj, features, n_type):\n",
    "    df_n = df_n.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "    df_n['JOB_SEEKER_ID'] = df_n['JOB_SEEKER_ID'].astype('int64')\n",
    "    # merging df_n and df_apj: Participants feature data with their num/den counts\n",
    "    \n",
    "    df_merge = df_apj.merge(df_n, on=['JOB_SEEKER_ID'], how='inner')\n",
    "    df_merge['JOB_SEEKER_ID'] = df_merge['JOB_SEEKER_ID'].astype('int64')\n",
    "    # check class imbalance:\n",
    "    df_sub = df_merge\n",
    "    num_n = str(n_type)\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    # running the Binary Logistic Regression (Num_n = 0, or Num_n = 1)\n",
    "    #separation of the features:\n",
    "    x = df_sub.loc[:, features].values\n",
    "    # target (Num_13)\n",
    "    y = df_sub[num_n]\n",
    "    \n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16, stratify=y)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\n",
    "    \n",
    "    adasyn = ADASYN(sampling_strategy=1, random_state=12)\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy=1, random_state=12)\n",
    "\n",
    "    # Create pipeline to apply both over-sampling and under-sampling\n",
    "    pipeline = Pipeline([('over', adasyn), ('under', rus)])\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Standardize the training and testing datasets\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_resampled)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # Hyperparameter tuning for XGBoost\n",
    "    params = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 9, 12],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.2],  # Added regularization parameter\n",
    "        'reg_lambda': [0.8, 1, 1.2]  # Added regularization parameter\n",
    "    }\n",
    "\n",
    "    clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    model_cv = RandomizedSearchCV(estimator=clf,\n",
    "                                  param_distributions=params,\n",
    "                                  n_iter=50,\n",
    "                                  scoring='accuracy',\n",
    "                                  cv=5,\n",
    "                                  verbose=3,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "    model_cv.fit(X_resampled, y_resampled)\n",
    "    best_params = model_cv.best_params_\n",
    "\n",
    "    # Instantiate XGBoost with best parameters\n",
    "    xgb_clf = xgb.XGBClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                learning_rate=best_params['learning_rate'],\n",
    "                                max_depth=best_params['max_depth'],\n",
    "                                min_child_weight=best_params['min_child_weight'],\n",
    "                                subsample=best_params['subsample'],\n",
    "                                gamma=best_params['gamma'],\n",
    "                                colsample_bytree=best_params['colsample_bytree'],\n",
    "                                reg_alpha=best_params['reg_alpha'],  # Added regularization parameter\n",
    "                                reg_lambda=best_params['reg_lambda'],  # Added regularization parameter\n",
    "                                random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    xgb_clf.fit(X_resampled, y_resampled, early_stopping_rounds=10, eval_set=[(X_test, y_test)])\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "     # Evaluate the model\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    df_merge = df_merge[['JOB_SEEKER_ID','CURRENT_CAPACITY_HOURS', 'GENDER', 'TIME_IN_SITE', 'DISABILITY_TYPE', 'AGE', 'INDIGENOUS_IND', 'IND_METRO']]\n",
    "    type_dict = {'CURRENT_CAPACITY_HOURS' : float, 'GENDER' : float, 'TIME_IN_SITE' : float, 'DISABILITY_TYPE' : float, 'AGE' : float, 'INDIGENOUS_IND' : float, 'IND_METRO' : float}\n",
    "    df_merge = df_merge.astype(type_dict)\n",
    "    \n",
    "    return cnf_matrix, class_report, df_merge, xgb_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub = ['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Num_13_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Num_13_full'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AManalo\\star_ratings_new\\Stat Regression\\stat_regression.ipynb Cell 146\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnf_matrix_13, class_report_13, df_merge_13, xg_model_13 \u001b[39m=\u001b[39m xgboost_model(df_13, df_sub, features_sub, \u001b[39m'\u001b[39;49m\u001b[39mNum_13_full\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\AManalo\\star_ratings_new\\Stat Regression\\stat_regression.ipynb Cell 146\u001b[0m in \u001b[0;36mxgboost_model\u001b[1;34m(df_n, df_apj, features, n_type)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m df_sub\u001b[39m.\u001b[39mloc[:, features]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# target (Num_13)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m y \u001b[39m=\u001b[39m df_sub[num_n]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, stratify\u001b[39m=\u001b[39my)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AManalo/star_ratings_new/Stat%20Regression/stat_regression.ipynb#Y265sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=16)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Num_13_full'"
     ]
    }
   ],
   "source": [
    "cnf_matrix_13, class_report_13, df_merge_13, xg_model_13 = xgboost_model(df_13, df_sub, features_sub, 'Num_13_full')\n",
    "# passing each dataframe row of df_merge_13 to the model.pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       197\n",
      "           1       0.61      1.00      0.76       313\n",
      "\n",
      "    accuracy                           0.61       510\n",
      "   macro avg       0.31      0.50      0.38       510\n",
      "weighted avg       0.38      0.61      0.47       510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(class_report_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN 13 week outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_13_pre.csv\")\n",
    "df_13_full = df_13[[\"Job_Seeker_ID\", \"Num_13\", \"Den_13\"]]\n",
    "df_13_full = df_13_full.dropna()\n",
    "\n",
    "df_13_full = df_13_full.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_13_full['Num_13_full_exp'] = 0\n",
    "# merge with df_merge_13\n",
    "df_13_full_m = df_sub.merge(df_13_full, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_13_full_m = df_13_full_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\", \"WAGE_SUBSIDY_ELIGIBLE\"]]\n",
    "df_13_full_m = df_13_full_m.set_index(keys=['JOB_SEEKER_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5069033530571992\n",
      "[[135  79]\n",
      " [171 122]]\n"
     ]
    }
   ],
   "source": [
    "best_model, X_train, X_test, y_resampled, y_test, scaler_knn_13_full = knn_regression(df_13_full, df_sub, features_sub, 'Num_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CURRENT_CAPACITY_HOURS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>TIME_IN_SITE</th>\n",
       "      <th>DISABILITY_TYPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INDIGENOUS_IND</th>\n",
       "      <th>IND_METRO</th>\n",
       "      <th>WAGE_SUBSIDY_ELIGIBLE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.684700e+05</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.091045e+09</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.690339e+09</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.641014e+09</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.549240e+06</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.878234e+09</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.332235e+09</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>530.0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.562966e+09</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.945749e+07</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.209370e+09</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1687 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              CURRENT_CAPACITY_HOURS GENDER  TIME_IN_SITE  DISABILITY_TYPE  \\\n",
       "JOB_SEEKER_ID                                                                \n",
       "1.684700e+05                      11      1         800.0                1   \n",
       "3.091045e+09                       8      0         530.0                1   \n",
       "9.690339e+09                      11      1        1009.0                1   \n",
       "2.641014e+09                      18      1         106.0                1   \n",
       "3.549240e+06                      18      1        1223.0                0   \n",
       "...                              ...    ...           ...              ...   \n",
       "2.878234e+09                      18      0         581.0                1   \n",
       "8.332235e+09                      18      1         530.0                1   \n",
       "8.562966e+09                       3      0         552.0                1   \n",
       "1.945749e+07                      18      1         434.0                1   \n",
       "8.209370e+09                      18      0          85.0                1   \n",
       "\n",
       "               AGE  INDIGENOUS_IND  IND_METRO  WAGE_SUBSIDY_ELIGIBLE  \n",
       "JOB_SEEKER_ID                                                         \n",
       "1.684700e+05    22               0          0                      1  \n",
       "3.091045e+09    26               0          0                      0  \n",
       "9.690339e+09    51               0          0                      0  \n",
       "2.641014e+09    37               0          0                      0  \n",
       "3.549240e+06    37               0          0                      1  \n",
       "...            ...             ...        ...                    ...  \n",
       "2.878234e+09    54               0          0                      1  \n",
       "8.332235e+09    44               0          0                      1  \n",
       "8.562966e+09    55               0          0                      0  \n",
       "1.945749e+07    19               0          0                      0  \n",
       "8.209370e+09    27               0          0                      1  \n",
       "\n",
       "[1687 rows x 8 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_13_full_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_13</th>\n",
       "      <th>Den_13</th>\n",
       "      <th>Num_13_full_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386301003</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653691003</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870290004</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498977509</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638331004</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836891309</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771045004</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325790</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690092007</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766520</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4785 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Num_13  Den_13  Num_13_full_exp\n",
       "JOB_SEEKER_ID                                 \n",
       "9386301003          0     1.0                0\n",
       "8653691003          0     1.0                1\n",
       "7870290004          0     1.0                0\n",
       "6498977509          1     1.0                1\n",
       "6638331004          0     1.0                0\n",
       "...               ...     ...              ...\n",
       "2836891309          1     1.0                0\n",
       "8771045004          0     1.0                0\n",
       "15325790            1     1.0                1\n",
       "690092007           1     1.0                0\n",
       "5766520             1     1.0                0\n",
       "\n",
       "[4785 rows x 3 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_13_full = df_13_full.set_index(keys=['JOB_SEEKER_ID'])\n",
    "\n",
    "for index, row in df_13_full_m.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    \n",
    "    # Apply the same scaler used during training\n",
    "    X_test_scaled = scaler_knn_13_full.transform(X_test)\n",
    "    \n",
    "    df_13_full.loc[index, 'Num_13_full_exp'] = best_model.predict(X_test_scaled)\n",
    "\n",
    "df_13_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 26 week outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26 week outcome\n",
    "### 26 week\n",
    "df_26 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_26_new.csv\")\n",
    "df_26_full = df_26[[\"Job_Seeker_ID\", \"Num_26_full\", \"Den_26_full\"]]\n",
    "df_26_full = df_26_full.dropna()\n",
    "\n",
    "df_26_full = df_26_full.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_26_full['Num_26_full_exp'] = 0\n",
    "# merge with df_merge_26\n",
    "df_26_full_m = df_sub.merge(df_26_full, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_26_full_m = df_26_full_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\", \"WAGE_SUBSIDY_ELIGIBLE\"]]\n",
    "\n",
    "\n",
    "\n",
    "df_26_path = df_26[[\"Job_Seeker_ID\", \"Num_26_path\", \"Den_26_path\"]]\n",
    "df_26_path = df_26_path.dropna()\n",
    "df_26_path = df_26_path.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_26_path['Num_26_path_exp'] = 0\n",
    "# merge with df_merge_26\n",
    "df_26_path_m = df_sub.merge(df_26_path, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_26_path_m = df_26_path_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\", \"WAGE_SUBSIDY_ELIGIBLE\"]]\n",
    "\n",
    "\n",
    "df_26_wrkast = df_26[[\"Job_Seeker_ID\", \"Num_26_wrkast\", \"Den_26_wrkast\"]]\n",
    "df_26_wrkast = df_26_wrkast.dropna()\n",
    "df_26_wrkast['Num_26_wrkast_exp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sub = ['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.615686274509804\n",
      "[[245  86]\n",
      " [110  69]]\n"
     ]
    }
   ],
   "source": [
    "best_model, X_train, X_test, y_resampled, y_test, scaler_knn_26_full = knn_regression(df_26_full, df_sub, features_sub, 'Num_26_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_26_full</th>\n",
       "      <th>Den_26_full</th>\n",
       "      <th>Num_26_full_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386301003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653691003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870290004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498977509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638331004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836891309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771045004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325790</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690092007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766520</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Num_26_full  Den_26_full  Num_26_full_exp\n",
       "JOB_SEEKER_ID                                           \n",
       "9386301003             0.0          0.0                0\n",
       "8653691003             0.0          1.0                0\n",
       "7870290004             0.0          1.0                0\n",
       "6498977509             1.0          1.0                1\n",
       "6638331004             1.0          1.0                0\n",
       "...                    ...          ...              ...\n",
       "2836891309             0.0          0.0                1\n",
       "8771045004             0.0          1.0                0\n",
       "15325790               0.0          0.0                0\n",
       "690092007              0.0          0.0                0\n",
       "5766520                0.0          0.0                0\n",
       "\n",
       "[4970 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_26_full_m = df_26_full_m.set_index(keys=['JOB_SEEKER_ID'])\n",
    "df_26_full = df_26_full.set_index(keys=['JOB_SEEKER_ID'])\n",
    "for index, row in df_26_full_m.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    \n",
    "    # Apply the same scaler used during training\n",
    "    X_test_scaled = scaler_knn_26_full.transform(X_test)\n",
    "    \n",
    "    df_26_full.loc[index, 'Num_26_full_exp'] = best_model.predict(X_test_scaled)\n",
    "\n",
    "df_26_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n",
      "[[476  27]\n",
      " [  7   0]]\n"
     ]
    }
   ],
   "source": [
    "best_model, X_train, X_test, y_resampled, y_test, scaler_knn_26_path = knn_regression(df_26_path, df_sub, features_sub, 'Num_26_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_path_m = df_26_path_m.set_index(keys=['JOB_SEEKER_ID'])\n",
    "df_26_path = df_26_path.set_index(keys=['JOB_SEEKER_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_26_path</th>\n",
       "      <th>Den_26_path</th>\n",
       "      <th>Num_26_path_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386301003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653691003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870290004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498977509</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638331004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836891309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771045004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325790</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690092007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766520</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Num_26_path  Den_26_path  Num_26_path_exp\n",
       "JOB_SEEKER_ID                                           \n",
       "9386301003             0.0          0.0                0\n",
       "8653691003             0.0          1.0                0\n",
       "7870290004             0.0          1.0                0\n",
       "6498977509             0.0          0.0                0\n",
       "6638331004             0.0          0.0                0\n",
       "...                    ...          ...              ...\n",
       "2836891309             0.0          0.0                0\n",
       "8771045004             0.0          1.0                0\n",
       "15325790               0.0          0.0                0\n",
       "690092007              0.0          0.0                1\n",
       "5766520                0.0          0.0                0\n",
       "\n",
       "[4970 rows x 3 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for index, row in df_26_path_m.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    \n",
    "    # Apply the same scaler used during training\n",
    "    X_test_scaled = scaler_knn_26_path.transform(X_test)\n",
    "    \n",
    "    df_26_path.loc[index, 'Num_26_path_exp'] = best_model.predict(X_test_scaled)\n",
    "\n",
    "df_26_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - 52 Week outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_52 = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/df_52_new.csv\")\n",
    "df_52_full = df_52[[\"Job_Seeker_ID\", \"Num_52_full\", \"Den_52_full\"]]\n",
    "df_52_full = df_52_full.dropna()\n",
    "df_52_path = df_52[[\"Job_Seeker_ID\", \"Num_52_path\", \"Den_52_path\"]]\n",
    "df_52_path = df_52_path.dropna()\n",
    "\n",
    "\n",
    "df_52_full = df_52_full.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_52_full['Num_52_full_exp'] = 0\n",
    "# merge with df_merge_52\n",
    "df_52_full_m = df_sub.merge(df_52_full, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_52_full_m = df_52_full_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\", \"WAGE_SUBSIDY_ELIGIBLE\"]]\n",
    "df_52_full_m = df_52_full_m.set_index(keys=['JOB_SEEKER_ID'])\n",
    "\n",
    "df_52_path = df_52_path.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "\n",
    "df_52_path['Num_52_path_exp'] = 0\n",
    "# merge with df_merge_52\n",
    "df_52_path_m = df_sub.merge(df_52_path, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n",
    "df_52_path_m = df_52_path_m[[\"JOB_SEEKER_ID\", \"CURRENT_CAPACITY_HOURS\", \"GENDER\", \"TIME_IN_SITE\", \"DISABILITY_TYPE\", \"AGE\", \"INDIGENOUS_IND\", \"IND_METRO\", \"WAGE_SUBSIDY_ELIGIBLE\"]]\n",
    "df_52_path_m = df_52_path_m.set_index(keys=['JOB_SEEKER_ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 52 full exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6745098039215687\n",
      "[[320 143]\n",
      " [ 23  24]]\n"
     ]
    }
   ],
   "source": [
    "best_model, X_train, X_test, y_resampled, y_test, scaler_knn_full = knn_regression(df_52_full, df_sub, features_sub, 'Num_52_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_52_full</th>\n",
       "      <th>Den_52_full</th>\n",
       "      <th>Num_52_full_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386301003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653691003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870290004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498977509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638331004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836891309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771045004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325790</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690092007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766520</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Num_52_full  Den_52_full  Num_52_full_exp\n",
       "JOB_SEEKER_ID                                           \n",
       "9386301003             0.0          0.0                0\n",
       "8653691003             0.0          1.0                0\n",
       "7870290004             0.0          1.0                0\n",
       "6498977509             1.0          1.0                1\n",
       "6638331004             0.0          0.0                0\n",
       "...                    ...          ...              ...\n",
       "2836891309             0.0          0.0                1\n",
       "8771045004             0.0          1.0                0\n",
       "15325790               0.0          0.0                0\n",
       "690092007              0.0          0.0                0\n",
       "5766520                0.0          0.0                0\n",
       "\n",
       "[4970 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_52_full = df_52_full.set_index(keys=['JOB_SEEKER_ID'])\n",
    "\n",
    "for index, row in df_52_full_m.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    \n",
    "    # Apply the same scaler used during training\n",
    "    X_test_scaled = scaler_knn_full.transform(X_test)\n",
    "    \n",
    "    df_52_full.loc[index, 'Num_52_full_exp'] = best_model.predict(X_test_scaled)\n",
    "\n",
    "df_52_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 52 path exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9862745098039216\n",
      "[[503   7]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "best_model, X_train, X_test, y_resampled, y_test, scaler_knn_path = knn_regression(df_52_path, df_sub, features_sub, 'Num_52_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_52_path</th>\n",
       "      <th>Den_52_path</th>\n",
       "      <th>Num_52_path_exp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9386301003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653691003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7870290004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498977509</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638331004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836891309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771045004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325790</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690092007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766520</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Num_52_path  Den_52_path  Num_52_path_exp\n",
       "JOB_SEEKER_ID                                           \n",
       "9386301003             0.0          0.0                0\n",
       "8653691003             0.0          0.0                0\n",
       "7870290004             0.0          0.0                0\n",
       "6498977509             0.0          0.0                0\n",
       "6638331004             0.0          0.0                0\n",
       "...                    ...          ...              ...\n",
       "2836891309             0.0          0.0                0\n",
       "8771045004             0.0          0.0                0\n",
       "15325790               0.0          0.0                0\n",
       "690092007              0.0          0.0                0\n",
       "5766520                0.0          0.0                0\n",
       "\n",
       "[4970 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_52_path = df_52_path.set_index(keys=['JOB_SEEKER_ID'])\n",
    "for index, row in df_52_path_m.iterrows():\n",
    "    X_test = pd.Series(row[['CURRENT_CAPACITY_HOURS', 'DISABILITY_TYPE', 'AGE', 'WAGE_SUBSIDY_ELIGIBLE']])\n",
    "    X_test = X_test.values.reshape(1, -1)\n",
    "     # Apply the same scaler used during training\n",
    "    \n",
    "    # Apply the same scaler used during training\n",
    "    X_test_scaled = scaler_knn_path.transform(X_test)\n",
    "    \n",
    "    df_52_path.loc[index, 'Num_52_path_exp'] = best_model.predict(X_test_scaled)\n",
    "\n",
    "df_52_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Performance using Stat Regression Model\n",
    "- No. of participants (NOT the no. of times they denominate) in the DEN post-quarterisation per performance measure \n",
    "- Expected = no. of outcomes (participants in the numerator)\n",
    "- Per contract per performance measure \n",
    "\n",
    "\n",
    "Method: \n",
    "1. Group data by jobseeker -> site -> contract \n",
    "- Bring in an additional dataframe that contains site/contract info.\n",
    "- Merge using job_seeker_id to df_merge\n",
    "- Aggregate into a groupBy object\n",
    "2. Create a separate test set without labels ('1') for numerator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating into a groupBy object\n",
    "- by site -> contract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Program                          0\n",
       "ess_identifier                   0\n",
       "Contract ID                    220\n",
       "Site Code                      126\n",
       "Site Description               126\n",
       "ESA Code                         0\n",
       "ESA Name                       126\n",
       "State                          126\n",
       "specialist_site_type_code    11683\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in the site/contract dataframe \n",
    "df_scg  = pd.read_csv(\"C:/Users/AManalo/star_ratings_new/site_contract_groups.csv\")\n",
    "print(len(df_scg))\n",
    "df_scg.dtypes\n",
    "\n",
    "# pre-processing the dataframe\n",
    "df_scg.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns\n",
    "df_scg = df_scg.rename(columns={'ESA Code' : 'esa_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Site Code\n",
    "2. Site Description \n",
    "can both be null -> as these both generate unique rows in the government released star ratings file. For the sake of data type conversion -> Empty values are encoded as 'XXXX' \n",
    "\n",
    "NOT NULL - defines each group (keys of the group)\n",
    "1. Contract ID\n",
    "2. ESA Code\n",
    "3. Speciality_site_type_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg[df_scg['Contract ID'] == '0212822B'].to_csv('missing_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg = df_scg[df_scg['Contract ID'].isna() == False]\n",
    "df_scg = df_scg[df_scg['specialist_site_type_code'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg['Site Code'] = df_scg['Site Code'].fillna(value='XXXX')\n",
    "df_scg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg = df_scg.rename(columns={'ess_identifier' : 'JOB_SEEKER_ID', 'Site Code' : 'SITE_CODE', 'specialist_site_type_code' : 'SPECIALIST_SITE_TYPE_CODE', 'Site Description' : 'SITE_DESCRIPTION', 'Contract ID' : 'CONTRACT_ID' })\n",
    "df_scg['JOB_SEEKER_ID'] = df_scg['JOB_SEEKER_ID'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining to df_merge\n",
    "- RIGHT join as the grouping needs to exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking the number of unique job seeker ids\n",
    "as stored in each performance measure dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_13 no. of Job Seekers: ' + str(len(df_13)))\n",
    "print('df_26 no. of Job Seekers: ' + str(len(df_26)))\n",
    "print('df_52 no. of Job Seekers: ' + str(len(df_52)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13.columns\n",
    "df_26 = df_26.rename(columns={'Job_Seeker_ID' :'JOB_SEEKER_ID'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_52.columns\n",
    "df_52 = df_52.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging pre-quarterisation dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_full = df_13_full.reset_index()\n",
    "df_26_full = df_26_full.reset_index()\n",
    "df_26_full = df_26_full.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_path = df_26_path.reset_index()\n",
    "df_52_path = df_52_path.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_wrkast = df_26_wrkast.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1 = df_scg.merge(df_13_full, on=['JOB_SEEKER_ID'], how='left')\n",
    "df_merge2 = df_merge1.merge(df_26_full, on=['JOB_SEEKER_ID'], how='left')\n",
    "df_merge3 = df_merge2.merge(df_26_path, on=['JOB_SEEKER_ID'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Program</th>\n",
       "      <th>JOB_SEEKER_ID</th>\n",
       "      <th>CONTRACT_ID</th>\n",
       "      <th>SITE_CODE</th>\n",
       "      <th>SITE_DESCRIPTION</th>\n",
       "      <th>ESA Code</th>\n",
       "      <th>ESA Name</th>\n",
       "      <th>State</th>\n",
       "      <th>SPECIALIST_SITE_TYPE_CODE</th>\n",
       "      <th>Num_13</th>\n",
       "      <th>Den_13</th>\n",
       "      <th>Num_13_full_exp</th>\n",
       "      <th>index</th>\n",
       "      <th>Num_26_full</th>\n",
       "      <th>Den_26_full</th>\n",
       "      <th>Num_26_full_exp</th>\n",
       "      <th>Num_26_path</th>\n",
       "      <th>Den_26_path</th>\n",
       "      <th>Num_26_path_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESS</td>\n",
       "      <td>168470</td>\n",
       "      <td>0212898K</td>\n",
       "      <td>AB31</td>\n",
       "      <td>AimBig Employment AUBURN</td>\n",
       "      <td>4CWS</td>\n",
       "      <td>Northern Sydney</td>\n",
       "      <td>NSW</td>\n",
       "      <td>AALL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DMS</td>\n",
       "      <td>451780</td>\n",
       "      <td>0212875D</td>\n",
       "      <td>MO37</td>\n",
       "      <td>AimBig Employment CALOUNDRA</td>\n",
       "      <td>4SUC</td>\n",
       "      <td>QLD North</td>\n",
       "      <td>QLD</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1798.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DMS</td>\n",
       "      <td>3606070</td>\n",
       "      <td>0212846K</td>\n",
       "      <td>MM07</td>\n",
       "      <td>AimBig Employment BURWOOD</td>\n",
       "      <td>4INW</td>\n",
       "      <td>South East Sydney</td>\n",
       "      <td>NSW</td>\n",
       "      <td>MENH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMS</td>\n",
       "      <td>618010</td>\n",
       "      <td>0212823C</td>\n",
       "      <td>MN07</td>\n",
       "      <td>AimBig Employment SUNSHINE</td>\n",
       "      <td>4WES</td>\n",
       "      <td>Western Victoria</td>\n",
       "      <td>VIC</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DMS</td>\n",
       "      <td>630040</td>\n",
       "      <td>0212831C</td>\n",
       "      <td>MM67</td>\n",
       "      <td>AimBig Employment ROCKINGHAM</td>\n",
       "      <td>4CWM</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>WA</td>\n",
       "      <td>MENH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5308</th>\n",
       "      <td>DMS</td>\n",
       "      <td>2730016019</td>\n",
       "      <td>0212841E</td>\n",
       "      <td>Y947</td>\n",
       "      <td>AimBig Employment MERRYLANDS</td>\n",
       "      <td>4CWS</td>\n",
       "      <td>Northern Sydney</td>\n",
       "      <td>NSW</td>\n",
       "      <td>AALL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5309</th>\n",
       "      <td>DMS</td>\n",
       "      <td>2696040</td>\n",
       "      <td>0212857B</td>\n",
       "      <td>ML77</td>\n",
       "      <td>AimBig Employment WOLLONGONG</td>\n",
       "      <td>4WOL</td>\n",
       "      <td>South East Sydney</td>\n",
       "      <td>NSW</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>DMS</td>\n",
       "      <td>5892574007</td>\n",
       "      <td>0212825E</td>\n",
       "      <td>MM77</td>\n",
       "      <td>AimBig Employment MELBOURNE</td>\n",
       "      <td>4YAR</td>\n",
       "      <td>Northern Victoria</td>\n",
       "      <td>VIC</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>DMS</td>\n",
       "      <td>832617003</td>\n",
       "      <td>0212823C</td>\n",
       "      <td>YA97</td>\n",
       "      <td>AimBig Employment Pty Ltd ST ALBANS</td>\n",
       "      <td>4WES</td>\n",
       "      <td>Western Victoria</td>\n",
       "      <td>VIC</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>DMS</td>\n",
       "      <td>2787935019</td>\n",
       "      <td>0212804K</td>\n",
       "      <td>MN97</td>\n",
       "      <td>AimBig Employment AITKENVALE</td>\n",
       "      <td>4TOW</td>\n",
       "      <td>QLD North</td>\n",
       "      <td>QLD</td>\n",
       "      <td>MENH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5313 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Program  JOB_SEEKER_ID CONTRACT_ID SITE_CODE  \\\n",
       "0        ESS         168470    0212898K      AB31   \n",
       "1        DMS         451780    0212875D      MO37   \n",
       "2        DMS        3606070    0212846K      MM07   \n",
       "3        DMS         618010    0212823C      MN07   \n",
       "4        DMS         630040    0212831C      MM67   \n",
       "...      ...            ...         ...       ...   \n",
       "5308     DMS     2730016019    0212841E      Y947   \n",
       "5309     DMS        2696040    0212857B      ML77   \n",
       "5310     DMS     5892574007    0212825E      MM77   \n",
       "5311     DMS      832617003    0212823C      YA97   \n",
       "5312     DMS     2787935019    0212804K      MN97   \n",
       "\n",
       "                         SITE_DESCRIPTION ESA Code           ESA Name State  \\\n",
       "0                AimBig Employment AUBURN     4CWS    Northern Sydney   NSW   \n",
       "1             AimBig Employment CALOUNDRA     4SUC          QLD North   QLD   \n",
       "2               AimBig Employment BURWOOD     4INW  South East Sydney   NSW   \n",
       "3              AimBig Employment SUNSHINE     4WES   Western Victoria   VIC   \n",
       "4            AimBig Employment ROCKINGHAM     4CWM  Western Australia    WA   \n",
       "...                                   ...      ...                ...   ...   \n",
       "5308         AimBig Employment MERRYLANDS     4CWS    Northern Sydney   NSW   \n",
       "5309         AimBig Employment WOLLONGONG     4WOL  South East Sydney   NSW   \n",
       "5310          AimBig Employment MELBOURNE     4YAR  Northern Victoria   VIC   \n",
       "5311  AimBig Employment Pty Ltd ST ALBANS     4WES   Western Victoria   VIC   \n",
       "5312         AimBig Employment AITKENVALE     4TOW          QLD North   QLD   \n",
       "\n",
       "     SPECIALIST_SITE_TYPE_CODE  Num_13  Den_13  Num_13_full_exp   index  \\\n",
       "0                         AALL     1.0     1.0              0.0   328.0   \n",
       "1                         MUSK     0.0     1.0              1.0  1798.0   \n",
       "2                         MENH     1.0     1.0              0.0   637.0   \n",
       "3                         MUSK     NaN     NaN              NaN     NaN   \n",
       "4                         MENH     1.0     1.0              0.0  3240.0   \n",
       "...                        ...     ...     ...              ...     ...   \n",
       "5308                      AALL     NaN     NaN              NaN     NaN   \n",
       "5309                      MUSK     NaN     NaN              NaN     NaN   \n",
       "5310                      MUSK     NaN     NaN              NaN     NaN   \n",
       "5311                      MUSK     NaN     NaN              NaN     NaN   \n",
       "5312                      MENH     NaN     NaN              NaN     NaN   \n",
       "\n",
       "      Num_26_full  Den_26_full  Num_26_full_exp  Num_26_path  Den_26_path  \\\n",
       "0             1.0          1.0              0.0          0.0          0.0   \n",
       "1             1.0          1.0              1.0          0.0          0.0   \n",
       "2             1.0          1.0              0.0          0.0          0.0   \n",
       "3             NaN          NaN              NaN          NaN          NaN   \n",
       "4             1.0          1.0              1.0          0.0          0.0   \n",
       "...           ...          ...              ...          ...          ...   \n",
       "5308          NaN          NaN              NaN          NaN          NaN   \n",
       "5309          NaN          NaN              NaN          NaN          NaN   \n",
       "5310          NaN          NaN              NaN          NaN          NaN   \n",
       "5311          NaN          NaN              NaN          NaN          NaN   \n",
       "5312          NaN          NaN              NaN          NaN          NaN   \n",
       "\n",
       "      Num_26_path_exp  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 NaN  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "5308              NaN  \n",
       "5309              NaN  \n",
       "5310              NaN  \n",
       "5311              NaN  \n",
       "5312              NaN  \n",
       "\n",
       "[5313 rows x 19 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Seeker_ID</th>\n",
       "      <th>Num_26_wrkast</th>\n",
       "      <th>Den_26_wrkast</th>\n",
       "      <th>Num_26_wrkast_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9386301003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8653691003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7870290004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6498977509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6638331004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965</th>\n",
       "      <td>2836891309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>8771045004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4967</th>\n",
       "      <td>15325790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>690092007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>5766520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4970 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Job_Seeker_ID  Num_26_wrkast  Den_26_wrkast  Num_26_wrkast_exp\n",
       "0        9386301003            0.0            0.0                  0\n",
       "1        8653691003            0.0            0.0                  0\n",
       "2        7870290004            0.0            0.0                  0\n",
       "3        6498977509            0.0            0.0                  0\n",
       "4        6638331004            0.0            0.0                  0\n",
       "...             ...            ...            ...                ...\n",
       "4965     2836891309            0.0            0.0                  0\n",
       "4966     8771045004            0.0            0.0                  0\n",
       "4967       15325790            0.0            0.0                  0\n",
       "4968      690092007            0.0            0.0                  0\n",
       "4969        5766520            0.0            0.0                  0\n",
       "\n",
       "[4970 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of df_merge_final: 5313\n"
     ]
    }
   ],
   "source": [
    "df_merge4 = df_merge3.merge(df_26_wrkast, on=['JOB_SEEKER_ID'], how='left')\n",
    "df_merge5 = df_merge4.merge(df_52_full, on=['JOB_SEEKER_ID'], how='left')\n",
    "df_merge6 = df_merge5.merge(df_52_path, on=['JOB_SEEKER_ID'], how='left')\n",
    "df_merge_final = df_merge6\n",
    "\n",
    "\n",
    "print('length of df_merge_final: ' + str(len(df_merge_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_final = df_merge_final.dropna(how='any',thresh=7, axis=0) # thresh set to 7 (at least 1 Numerator/Denominator value not na)\n",
    "df_merge_final.to_csv('C:/Users/AManalo/star_ratings_new/df_merge_post.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in apv_stream_participant_versions \n",
    "\n",
    "df_apv = pd.read_csv('C:/Users/AManalo/OneDrive - Arriba Group/Desktop/Star Ratings - AimBig/apv_stream_participant_versions.csv')\n",
    "# joining to df_merge\n",
    "df_apv = df_apv.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "df_comb2 = df_apv.merge(df_scg, on=['JOB_SEEKER_ID'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging post-quarterisation dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting the post quarterisation dataframes to their subtypes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_13 = pd.read_csv('C:/Users/AManalo/star_ratings_new/df_13_post.csv')\n",
    "df_post_26 = pd.read_csv('C:/Users/AManalo/star_ratings_new/df_post_26.csv')\n",
    "df_post_52 = pd.read_csv('C:/Users/AManalo/star_ratings_new/df_post_52.csv')\n",
    "\n",
    "df_post_13 = df_post_13.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "df_post_13 = df_post_13.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_post_26 = df_post_26.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "df_post_26 = df_post_26.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_post_52 = df_post_52.rename(columns={'Job_Seeker_ID' : 'JOB_SEEKER_ID'})\n",
    "df_post_52 = df_post_52.drop(columns=['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging to site_contract_level - joining on job-seeker-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_26 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns for df_scg (site-level data) -> rename so that it is lowercare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg = df_scg.rename(columns={'CONTRACT_ID' : 'Contract_ID', 'SITE_DESCRIPTION' : 'Site_Name', 'SPECIALIST_SITE_TYPE_CODE' : 'specialist_site_type_code'})\n",
    "df_scg = df_scg.drop(columns=['Program', 'SITE_CODE', 'ESA_CODE', 'ESA Name', 'State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the dataframes together to site level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# creating merged data frame -> to pass to star_ratings_new_results\n",
    "df_merge1 = df_post_13.merge(df_post_26, on=['JOB_SEEKER_ID', 'Site_Name', 'Contract_ID', 'specialist_site_type_code'], how='inner')\n",
    "df_merge2 = df_merge1.merge(df_post_52, on=['JOB_SEEKER_ID', 'Site_Name', 'Contract_ID', 'specialist_site_type_code'], how='inner')\n",
    "\n",
    "df_merge_final = df_merge2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merge_final.to_csv('C:/Users/AManalo/star_ratings_new/df_merge_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected performance vs Denominator\n",
    "Denominator = no. of participants in caseload under each contract\n",
    "1. Creating a data frame to store all denominator/expected performance\n",
    "2. Populate the dataframe and calculated expected performance rate % \n",
    "Aggregate using groupBy attribute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_exp_per = pd.DataFrame(columns=['CONTRACT_ID', 'ESA_CODE', 'SPECIALIST_SITE_TYPE_CODE', 'DEN_13_FULL', 'EXPECTED_13', 'EXPECTED_PER_RATE_13', 'DENOMINATOR_26', 'EXPECTED_26', 'EXPECTED_PER_RATE_26', 'DENOMINATOR_52', 'EXPECTED_PER_RATE_52', 'EXPECTED_PER_RATE_52'])\n",
    "\n",
    "len(df_merge_final)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected performance for each job seeker ID\n",
    "- Then sum the expected performance -> contract level\n",
    "- Model coefficient and intercept will differ based on the input (training/test set)\n",
    "- Each equation is uniquely constructed based on the get_eqn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated above\n",
    "df_13_full['Num_13_full_exp'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_full['Num_26_full_exp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_26_path['Num_26_path_exp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_52_full['Num_52_full_exp'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0536884df65f5e39992ad2e5a4490020c7eea9ac64ea16b2fd494381cba4ac7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
